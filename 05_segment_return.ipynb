{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from string import punctuation, whitespace\n",
    "from ast import literal_eval\n",
    "import regex as re\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_directory = os.path.abspath('')\n",
    "\n",
    "relative_input_directory = 'outputs/intermediate/'\n",
    "data_input_path = os.path.join(cur_directory, relative_input_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually added in the missing text chunk (xlsx decided that the chunk was a giant URL)\n",
    "chunk = pd.read_excel(os.path.join(data_input_path, \"df_chunk 12.xlsx\"))\n",
    "long = pd.read_excel(os.path.join(data_input_path, \"gpt_responses_long_final_clean.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACF-2023-0011-0485-0']\n"
     ]
    }
   ],
   "source": [
    "# Determine if there are any IDs in the chunk file that didn't make it into the long file\n",
    "missing_lst = [x for x in chunk['text_chunk_id'] if x not in long['text_chunk_id'].unique()]\n",
    "print(missing_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Document', 'chunk', 'to_translate', 'all_text',\n",
       "       'validation', 'dont_run', 'Comment_counts', 'attachment_text_counts',\n",
       "       'text', 'text_chunk_id', 'topic_number', 'topic', 'segment_number',\n",
       "       'segment', 'intent_number', 'intent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_df = pd.DataFrame()\n",
    "for tid in missing_lst:\n",
    "    #add_one = chunk[chunk['text_chunk_id']=='ACF-2023-0011-0485-0']\n",
    "    add_one = chunk[chunk['text_chunk_id'] == tid]\n",
    "    add_one['topic_number'] =\"Missing Segment\"\n",
    "    add_one['topic'] =\"uncategorized\"\n",
    "    add_one['segment_number']=\"None\"\n",
    "    add_one['segment']= \"\"\n",
    "    add_one['intent_number'] =\"uncategorized\"\n",
    "    add_one['intent']=\"uncategorized\"\n",
    "    missing_df = pd.concat([missing_df, add_one])\n",
    "add_one.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "long = long[['text_chunk_id', 'Document', 'text',\n",
    "       'to_translate', 'validation', 'dont_run', 'Comment_counts',\n",
    "       'attachment_text_counts', 'topic_number', 'topic', 'segment_number',\n",
    "       'segment', 'intent_number', 'intent']]\n",
    "missing_df = missing_df[['text_chunk_id', 'Document', 'text',\n",
    "       'to_translate', 'validation', 'dont_run', 'Comment_counts',\n",
    "       'attachment_text_counts', 'topic_number', 'topic', 'segment_number',\n",
    "       'segment', 'intent_number', 'intent']]\n",
    "long = pd.concat([long, missing_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(long, short):\n",
    "    '''\n",
    "    Given a long and short input string, find the starting indices of the short\n",
    "    string within the long string.\n",
    "    Inputs\n",
    "        long : String\n",
    "        short : string\n",
    "    Returns \n",
    "        list of integers\n",
    "    '''\n",
    "    long = long.lower()\n",
    "    short = short.lower()\n",
    "    return [i for i in range(len(long)) if long.startswith(short, i)]\n",
    "\n",
    "def col_to_list(row):\n",
    "    ''' \n",
    "    Convert a string list (ex: \"['hello', 'hi']\") into a list\n",
    "    '''\n",
    "    if pd.isnull(row) or row == \"\" or row == \"[]\":\n",
    "        return []\n",
    "    else:\n",
    "        return literal_eval(row)\n",
    "\n",
    "def find_substring(relevant_inds, clean_short, long, fuzz_function=fuzz.ratio):\n",
    "    ''' \n",
    "    Given a list of indices, searches the long string to find the best match\n",
    "    to the short string\n",
    "    Inputs\n",
    "        relevant_inds : list of starting indices for possible matches\n",
    "        clean_short : string\n",
    "        long : string\n",
    "        fuzz_function : fuzzy matching function to use for matching\n",
    "    Returns\n",
    "        best_match : string\n",
    "    '''\n",
    "    best_match = \"\"\n",
    "    best_similarity = 0.0\n",
    "    for idx in relevant_inds:\n",
    "        long_substring = long[idx:idx+len(clean_short)]\n",
    "        similarity = fuzz_function(clean_short, long_substring)\n",
    "        if similarity == 100:\n",
    "            best_similarity = similarity\n",
    "            best_match = long_substring\n",
    "            break\n",
    "        elif similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_match = long_substring\n",
    "    if best_similarity < 90:\n",
    "        return False\n",
    "    return best_match\n",
    "    \n",
    "def find_best_matching_substring(long, short):\n",
    "    '''\n",
    "    Given a long string and a short string, uses fuzzy matching to find the \n",
    "    closest match to the short string within the long string.\n",
    "    Inputs:\n",
    "        long : string\n",
    "        short : string\n",
    "    Returns:\n",
    "        best_string : string\n",
    "    '''\n",
    "    clean_short = short.rstrip(whitespace + punctuation)\n",
    "    short_start = clean_short[:5].rstrip(whitespace + punctuation)\n",
    "    relevant_inds = find_indices(long, short_start)\n",
    "    if relevant_inds == []:\n",
    "        short_start = clean_short[:4].rstrip(whitespace + punctuation)\n",
    "        relevant_inds = find_indices(long, short_start)\n",
    "        if relevant_inds == []:\n",
    "            short_start = clean_short[:3].rstrip(whitespace + punctuation)\n",
    "            relevant_inds = find_indices(long, short_start)\n",
    "    best_string = find_substring(relevant_inds, clean_short, long)\n",
    "    if not best_string:\n",
    "        best_string = find_substring(relevant_inds, clean_short, long, fuzz.token_set_ratio)\n",
    "    return best_string\n",
    "\n",
    "def find_missing_segments(df1, df2):\n",
    "    '''\n",
    "    Given a chunked dataframe (df1) and a long dataframe(df2), determine which portions\n",
    "    of the chunk text are not represented in the long dataframe\n",
    "    Inputs:\n",
    "        df1 : chunk dataframe\n",
    "        df2 : long dataframe\n",
    "    Outputs:\n",
    "        df1_exp : New dataframe that includes columns indicating missing segments \n",
    "        problem_ids : List of text chunk ids where we couldn't find their matching substring\n",
    "        df2 : Adjusted long dataframe that includes some columns helpful for troubleshooting\n",
    "    '''\n",
    "    df1['missing_segments'] = \"\" \n",
    "    df1['flag_missing_segments']= 0\n",
    "    df1['flag_cant_find_sub'] = 0\n",
    "    problem_ids = []\n",
    "    for doc in df2['text_chunk_id'].unique():\n",
    "        df_seg = df2[df2['text_chunk_id']==doc]\n",
    "        df_chunk = df2[df2['text_chunk_id']==doc]\n",
    "        concat_segs = ' '.join(set(df_seg['segment']))\n",
    "        chunk = df_chunk['text'].iloc[0]\n",
    "        \n",
    "        fuzz_ratio = fuzz.ratio(concat_segs, chunk)\n",
    "        fuzz_tokensort = fuzz.token_sort_ratio(concat_segs, chunk)\n",
    "        if fuzz_ratio == 100 or fuzz_tokensort == 100:\n",
    "            continue\n",
    "        expanding_segs = ''\n",
    "        for segment in df_seg['segment'].unique():\n",
    "            if (segment.strip() in punctuation) or (chunk == 'HAVE FROM CHATGPT') or (segment in expanding_segs):\n",
    "                continue\n",
    "            best_substring = find_best_matching_substring(chunk, segment)\n",
    "            if not best_substring:\n",
    "                print(doc)\n",
    "                problem_ids.append(doc)\n",
    "                df2.loc[(df2['text_chunk_id']==doc) & (df2['segment']==segment), 'flag_cant_find_sub'] =1\n",
    "                continue\n",
    "            expanding_segs += best_substring\n",
    "            expanding_segs += segment\n",
    "\n",
    "            chunk = chunk.replace(best_substring[:-2], \"HAVE FROM CHATGPT\")\n",
    "        \n",
    "        final = chunk.split(\"HAVE FROM CHATGPT\")\n",
    "        final_clean = [x.rstrip(whitespace + punctuation) for x in final if any(let not in whitespace and let not in punctuation for let in x)]\n",
    "        final_clean2 = [x for x in final_clean if (x.lower().strip(punctuation + whitespace) not in expanding_segs.lower()) and (len(x) > 10)]\n",
    "        df1.loc[df1['text_chunk_id']==doc, 'missing_segments'] = str(final_clean2)\n",
    "        df1.loc[df1['text_chunk_id']==doc, 'flag_missing_segments'] = 1\n",
    "        df1.loc[df1['text_chunk_id']==doc, 'all_segments_apearing'] = str(concat_segs)\n",
    "        fuzzy_lst = []\n",
    "        for mseg in final_clean2:\n",
    "            fuzzy_lst.append(fuzz.partial_ratio(mseg, concat_segs))\n",
    "        df1.loc[df1['text_chunk_id']==doc, 'fuzz_ratio']= str(fuzzy_lst)\n",
    "\n",
    "    df1['missing_segments'] = df1['missing_segments'].apply(col_to_list) #convert to list type\n",
    "    df1['fuzz_ratio'] = df1['fuzz_ratio'].apply(col_to_list)\n",
    "    df1_exp = df1.explode([\"missing_segments\",\"fuzz_ratio\"])\n",
    "    df1_exp = df1_exp[df1_exp['flag_missing_segments']==1]\n",
    "    df1_exp['topic_number'] = 'Missing Segment'\n",
    "    df1_exp['topic'] = 'uncategorized'\n",
    "    df1_exp['segment_number'] = 'segment_1'\n",
    "    df1_exp['intent_number'] = 'uncategorized'\n",
    "    df1_exp['intent'] = 'uncategorized'\n",
    "\n",
    "    return df1_exp, problem_ids, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACF-2023-0011-0251-A1-0\n",
      "ACF-2023-0011-0251-A1-0\n",
      "ACF-2023-0011-0301-A1-9\n",
      "ACF-2023-0011-0377-A3-11\n",
      "ACF-2023-0011-0377-A3-11\n",
      "ACF-2023-0011-0377-A3-11\n",
      "ACF-2023-0011-0377-A3-12\n",
      "ACF-2023-0011-0377-A3-3\n",
      "ACF-2023-0011-0377-A3-4\n",
      "ACF-2023-0011-0377-A3-4\n",
      "ACF-2023-0011-0377-A3-6\n",
      "ACF-2023-0011-0377-A3-8\n",
      "ACF-2023-0011-0380-A3-15\n",
      "ACF-2023-0011-0409-A1-0\n",
      "ACF-2023-0011-0409-A1-1\n",
      "ACF-2023-0011-0409-A1-1\n",
      "ACF-2023-0011-0409-A1-2\n",
      "ACF-2023-0011-0409-A1-2\n",
      "ACF-2023-0011-0409-A1-2\n",
      "ACF-2023-0011-0409-A1-2\n",
      "ACF-2023-0011-0409-A1-2\n",
      "ACF-2023-0011-0409-A1-3\n",
      "ACF-2023-0011-0409-A1-4\n",
      "ACF-2023-0011-0409-A1-4\n",
      "ACF-2023-0011-0409-A1-5\n",
      "ACF-2023-0011-0409-A1-5\n",
      "ACF-2023-0011-0468-A1-0\n",
      "ACF-2023-0011-0515-A1-1\n",
      "ACF-2023-0011-0515-A1-6\n",
      "ACF-2023-0011-0568-0\n",
      "ACF-2023-0011-0586-A1-1\n",
      "ACF-2023-0011-0610-A1-3\n",
      "ACF-2023-0011-0630-A1-2\n",
      "ACF-2023-0011-0640-A1-2\n",
      "ACF-2023-0011-0640-A1-2\n",
      "ACF-2023-0011-0667-A2-1\n",
      "ACF-2023-0011-0726-A1-1\n",
      "ACF-2023-0011-0824-A1-0\n",
      "ACF-2023-0011-0913-A1-0\n",
      "ACF-2023-0011-0939-A1-1\n",
      "ACF-2023-0011-DRAFT-0954-A1-6\n",
      "ACF-2023-0011-DRAFT-1045-A1-0\n",
      "ACF-2023-0011-DRAFT-1045-A1-2\n",
      "ACF-2023-0011-DRAFT-1045-A1-2\n",
      "ACF-2023-0011-DRAFT-1053-A1-3\n",
      "ACF-2023-0011-DRAFT-1058-A1-0\n",
      "ACF-2023-0011-DRAFT-1069-A1-0\n",
      "ACF-2023-0011-DRAFT-1069-A1-1\n",
      "ACF-2023-0011-DRAFT-1069-A1-2\n",
      "ACF-2023-0011-DRAFT-1079-A1-6\n",
      "ACF-2023-0011-DRAFT-1146-A1-0\n",
      "ACF-2023-0011-DRAFT-1193-A1-0\n",
      "ACF-2023-0011-DRAFT-1193-A1-1\n",
      "ACF-2023-0011-DRAFT-1289-A1-0\n",
      "ACF-2023-0011-DRAFT-1289-A1-1\n",
      "ACF-2023-0011-DRAFT-1294-A1-0\n"
     ]
    }
   ],
   "source": [
    "missing_segments, segment_problem_ids, long2 = find_missing_segments(chunk, long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the missing segments\n",
    "missing_segments.to_excel(\"missing_segments_only.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how many segments we weren't able to find substring matches for\n",
    "len(set(segment_problem_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_substring(relevant_inds, clean_short, long, fuzz_function=fuzz.ratio):\n",
    "    '''Given a list of relevant indices, a short string, a long string, and a matching function,\n",
    "    return the best matching substring for short within long, and the starting index for that \n",
    "    substring, and the similarity score for that substring.'''\n",
    "    \n",
    "    best_match = \"\"\n",
    "    best_similarity = 0.0\n",
    "    best_idx = None\n",
    "\n",
    "    for idx in relevant_inds:\n",
    "        long_substring = long[idx:idx+len(clean_short)]\n",
    "\n",
    "        similarity = fuzz_function(clean_short, long_substring)\n",
    "        if similarity == 100:\n",
    "            best_similarity = similarity\n",
    "            best_match = long_substring\n",
    "            best_idx = idx\n",
    "            break\n",
    "        elif similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_match = long_substring\n",
    "            best_idx = idx\n",
    "\n",
    "    return (best_match, best_idx, best_similarity)\n",
    "    \n",
    "def find_best_matching_substring(long, short):\n",
    "    '''Given a long string and a short string, finds the best matching substring for the short \n",
    "    string within the long string. Finds the associated string index for that matching substring. \n",
    "    Finds the best similarity score for the associated substring'''\n",
    "\n",
    "    clean_short = short.strip(whitespace + punctuation)\n",
    "    short_start = clean_short[:5].strip(whitespace + punctuation)\n",
    "    \n",
    "    relevant_inds = find_indices(long, short_start)\n",
    "    if relevant_inds == []:\n",
    "        short_start = clean_short[:4].strip(whitespace + punctuation)\n",
    "        relevant_inds = find_indices(long, short_start)\n",
    "        if relevant_inds == []:\n",
    "            short_start = clean_short[:3].strip(whitespace + punctuation)\n",
    "            relevant_inds = find_indices(long, short_start)\n",
    "    best_string, best_idx, best_sim = find_substring(relevant_inds, clean_short, long)\n",
    "    if not best_string:\n",
    "        best_string, best_idx, best_sim = find_substring(relevant_inds, clean_short, long, fuzz.token_set_ratio)\n",
    "    return (best_string, best_idx, best_sim)\n",
    "\n",
    "def find_segment_indices(df):\n",
    "    '''Given a dataframe (df), returns the dataframe with two new columns identifing the \n",
    "    segment index number and the fuzz ratio to indicate the confidence of that index'''\n",
    "\n",
    "    for chunk_id in df['text_chunk_id'].unique():\n",
    "        chunk = df[df['text_chunk_id']==chunk_id].iloc[0]['text']\n",
    "        for seg in df[df['text_chunk_id']==chunk_id]['segment']:\n",
    "            if not pd.isna(seg):\n",
    "                _, idx, best_sim = find_best_matching_substring(chunk, seg)\n",
    "                df.loc[(df['segment']==seg) & (df['text_chunk_id']==chunk_id), 'segment_index'] = idx\n",
    "                df.loc[(df['segment']==seg) & (df['text_chunk_id']==chunk_id), 'fuzz_ratio_index'] = best_sim\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(os.path.join(data_input_path, \"gpt_responses_long_final_clean.xlsx\"))\n",
    "mdf = pd.read_excel(\"missing_segments_only.xlsx\")\n",
    "\n",
    "df = df[['Unnamed: 0', 'comment number', 'text_chunk_id', 'Document', 'text',\n",
    "       'to_translate', 'validation', 'dont_run', 'Comment_counts',\n",
    "       'attachment_text_counts', 'topic_number', 'topic', 'segment_number',\n",
    "       'segment', 'intent_number', 'intent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text_chunk_id', 'Document', 'text', 'to_translate', 'validation',\n",
       "       'dont_run', 'Comment_counts', 'attachment_text_counts', 'topic_number',\n",
       "       'topic', 'segment_number', 'segment', 'fuzz_ratio',\n",
       "       'all_segments_apearing', 'intent_number', 'intent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Align the long df and the missing df columns\n",
    "df['fuzz_ratio'] = None\n",
    "df['all_segments_apearing'] = None\n",
    "df = df[['text_chunk_id', 'Document', 'text',\n",
    "       'to_translate', 'validation', 'dont_run', 'Comment_counts',\n",
    "       'attachment_text_counts', 'topic_number', 'topic', 'segment_number',\n",
    "       'segment', 'fuzz_ratio', 'all_segments_apearing', 'intent_number', 'intent']]\n",
    "mdf = mdf[['text_chunk_id', 'Document', 'text',\n",
    "       'to_translate', 'validation', 'dont_run', 'Comment_counts',\n",
    "       'attachment_text_counts', 'topic_number', 'topic', 'segment_number',\n",
    "       'missing_segments', 'fuzz_ratio', 'all_segments_apearing', 'intent_number', 'intent']]\n",
    "mdf = mdf.rename(columns={\"missing_segments\":\"segment\"})\n",
    "mdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign indices for all segments\n",
    "mdf = find_segment_indices(mdf)\n",
    "df = find_segment_indices(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out blank segments from missing segment dataframe\n",
    "mdf = mdf[~pd.isna(mdf['segment'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1640\n"
     ]
    }
   ],
   "source": [
    "# Use regex to remove weird characters/short nonsense words from missing segments.\n",
    "pattern = re.compile(r'..\\.\\n')\n",
    "pattern2 = re.compile(r'^[a-zA-Z]{2}\\.')\n",
    "pattern3 = re.compile(r'^[a-zA-Z]{2}\\?')\n",
    "pattern4 = re.compile(r'^[a-zA-Z]{3}\\.')\n",
    "pattern4 = re.compile(r'^[a-zA-Z]{3}\\?')\n",
    "\n",
    "for i in mdf.index:\n",
    "    row = mdf.loc[i]\n",
    "    concat_segs = row['all_segments_apearing']\n",
    "    seg = row['segment']\n",
    "    sub_str_lst = pattern.findall(seg)\n",
    "    sub_str_lst.extend(pattern2.findall(seg))\n",
    "    sub_str_lst.extend(pattern3.findall(seg))\n",
    "    sub_str_lst.extend(pattern4.findall(seg))\n",
    "    for sub_str in sub_str_lst:\n",
    "        seg = seg.replace(sub_str, \"\")\n",
    "    mdf.loc[i,\"segment\"] = seg\n",
    "    try:\n",
    "        # Determine if the missing segment is likely already represented in the concatenated segments\n",
    "        ratio = fuzz.partial_ratio(seg.replace(\"\\n\", \" \"), concat_segs)\n",
    "        mdf.loc[i,\"fuzz_ratio\"] = ratio\n",
    "    except:\n",
    "        mdf.loc[i,\"fuzz_ratio\"] = 0\n",
    "# Remove missing segments that are likely not missing at all (have a high fuzz ratio)\n",
    "mdf = mdf[mdf['fuzz_ratio'] < 80] \n",
    "print(len(mdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdf.columns == df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19631\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df, mdf])\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"full_df_and_missing_segments_test.xlsx\")\n",
    "# Run manual check on blank segment indices and segment indices with very low fuzzy match ratio_index\n",
    "# Below code will only work if all \"segment_index\" cells are filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"full_df_and_missing_segments_test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ChatGPT Topic Match' 'Missing Segment']\n",
      "1294\n"
     ]
    }
   ],
   "source": [
    "df = df.rename(columns={\"topic_number\":\"topic_origin\"})\n",
    "df['rank'] = df.groupby('text_chunk_id')['segment_index'].rank(ascending=True, method='dense').astype(int)\n",
    "df.drop(\"segment_index\",axis=1)\n",
    "df = df.rename(columns={\"rank\":\"segment_index\"})\n",
    "df['topic_origin'] = np.where(df['topic_origin']!=\"Missing Segment\", \"ChatGPT Topic Match\", df['topic_origin'])\n",
    "print(df['topic_origin'].unique())\n",
    "df['segment_length'] = df['segment'].apply(lambda x: len(x.split(\" \")))\n",
    "df['segment_lessthan_30words'] = np.where(df['segment_length']<30,1,0)\n",
    "df['segment_lessthan_15words'] = np.where(df['segment_length']<15,1,0)\n",
    "print(df['segment_lessthan_15words'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"full_df_and_missing_segments5.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_tests-44WajXxm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
